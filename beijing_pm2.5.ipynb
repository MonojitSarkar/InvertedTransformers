{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install ucimlrepo","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-27T15:48:06.465269Z","iopub.execute_input":"2024-06-27T15:48:06.465638Z","iopub.status.idle":"2024-06-27T15:48:22.059321Z","shell.execute_reply.started":"2024-06-27T15:48:06.465611Z","shell.execute_reply":"2024-06-27T15:48:22.058041Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting ucimlrepo\n  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: pandas>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ucimlrepo) (2.2.1)\nRequirement already satisfied: certifi>=2020.12.5 in /opt/conda/lib/python3.10/site-packages (from ucimlrepo) (2024.2.2)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->ucimlrepo) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\nDownloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\nInstalling collected packages: ucimlrepo\nSuccessfully installed ucimlrepo-0.0.7\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from ucimlrepo import fetch_ucirepo \n  \n# fetch dataset \nbeijing_pm2_5 = fetch_ucirepo(id=381) \n  \n# data (as pandas dataframes) \nX = beijing_pm2_5.data.features \ny = beijing_pm2_5.data.targets \n  \n# metadata \nprint(beijing_pm2_5.metadata) \n  \n# variable information \nprint(beijing_pm2_5.variables) ","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:22.061393Z","iopub.execute_input":"2024-06-27T15:48:22.061724Z","iopub.status.idle":"2024-06-27T15:48:23.628519Z","shell.execute_reply.started":"2024-06-27T15:48:22.061694Z","shell.execute_reply":"2024-06-27T15:48:23.627340Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"{'uci_id': 381, 'name': 'Beijing PM2.5', 'repository_url': 'https://archive.ics.uci.edu/dataset/381/beijing+pm2+5+data', 'data_url': 'https://archive.ics.uci.edu/static/public/381/data.csv', 'abstract': 'This hourly data set contains the PM2.5 data of US Embassy in Beijing. Meanwhile, meteorological data from Beijing Capital International Airport are also included. ', 'area': 'Climate and Environment', 'tasks': ['Regression'], 'characteristics': ['Multivariate', 'Time-Series'], 'num_instances': 43824, 'num_features': 11, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['pm2.5'], 'index_col': ['No'], 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 2015, 'last_updated': 'Sat Mar 16 2024', 'dataset_doi': '10.24432/C5JS49', 'creators': ['Song Chen'], 'intro_paper': {'title': \"Assessing Beijing's PM2.5 pollution: severity, weather impact, APEC and winter heating\", 'authors': 'Xuan Liang, T. Zou, Bin Guo, Shuo Li, Haozhe Zhang, Shuyi Zhang, Hui Huang, S. Chen', 'published_in': 'Proceedings of the Royal Society A', 'year': 2015, 'url': 'https://www.semanticscholar.org/paper/8a82cccc111cbe3e7ff7bc16a3345afe8351a425', 'doi': '10.1098/rspa.2015.0257'}, 'additional_info': {'summary': 'The data\\'s time period is between Jan 1st, 2010 to Dec 31st, 2014. Missing data are denoted as \"NA\".', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'No: row number\\r\\nyear: year of data in this row\\r\\nmonth: month of data in this row\\r\\nday: day of data in this row\\r\\nhour: hour of data in this row\\r\\npm2.5: PM2.5 concentration (ug/m^3)\\r\\nDEWP: Dew Point (â„ƒ)\\r\\nTEMP: Temperature (â„ƒ)\\r\\nPRES: Pressure (hPa)\\r\\ncbwd: Combined wind direction\\r\\nIws: Cumulated wind speed (m/s)\\r\\nIs: Cumulated hours of snow\\r\\nIr: Cumulated hours of rain\\r\\n', 'citation': None}}\n     name     role         type demographic description units missing_values\n0      No       ID      Integer        None        None  None             no\n1    year  Feature      Integer        None        None  None             no\n2   month  Feature      Integer        None        None  None             no\n3     day  Feature      Integer        None        None  None             no\n4    hour  Feature      Integer        None        None  None             no\n5   pm2.5   Target      Integer        None        None  None            yes\n6    DEWP  Feature      Integer        None        None  None             no\n7    TEMP  Feature      Integer        None        None  None             no\n8    PRES  Feature      Integer        None        None  None             no\n9    cbwd  Feature  Categorical        None        None  None             no\n10    Iws  Feature   Continuous        None        None  None             no\n11     Is  Feature      Integer        None        None  None             no\n12     Ir  Feature      Integer        None        None  None             no\n","output_type":"stream"}]},{"cell_type":"code","source":"import datetime\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom IPython.core.debugger import set_trace","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:23.629706Z","iopub.execute_input":"2024-06-27T15:48:23.629985Z","iopub.status.idle":"2024-06-27T15:48:27.770516Z","shell.execute_reply.started":"2024-06-27T15:48:23.629960Z","shell.execute_reply":"2024-06-27T15:48:27.769371Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:27.773468Z","iopub.execute_input":"2024-06-27T15:48:27.773924Z","iopub.status.idle":"2024-06-27T15:48:27.814130Z","shell.execute_reply.started":"2024-06-27T15:48:27.773894Z","shell.execute_reply":"2024-06-27T15:48:27.813177Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"null_rows = y[y['pm2.5'].isnull()].index.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:27.815594Z","iopub.execute_input":"2024-06-27T15:48:27.816506Z","iopub.status.idle":"2024-06-27T15:48:27.824342Z","shell.execute_reply.started":"2024-06-27T15:48:27.816467Z","shell.execute_reply":"2024-06-27T15:48:27.823435Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"x = X.drop(labels=null_rows, axis=0)\ny = y.drop(labels=null_rows, axis=0)\nx.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:27.825657Z","iopub.execute_input":"2024-06-27T15:48:27.826070Z","iopub.status.idle":"2024-06-27T15:48:27.848679Z","shell.execute_reply.started":"2024-06-27T15:48:27.826033Z","shell.execute_reply":"2024-06-27T15:48:27.847525Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"((41757, 11), (41757, 1))"},"metadata":{}}]},{"cell_type":"code","source":"df = x.copy(deep=True)\n#df['datetime'] = pd.to_datetime(x[['year', 'month', 'day', 'hour']])\ndf = df.drop(labels=['year', 'month', 'day', 'hour'], axis=1)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:27.849901Z","iopub.execute_input":"2024-06-27T15:48:27.850264Z","iopub.status.idle":"2024-06-27T15:48:27.863044Z","shell.execute_reply.started":"2024-06-27T15:48:27.850237Z","shell.execute_reply":"2024-06-27T15:48:27.861910Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(41757, 7)"},"metadata":{}}]},{"cell_type":"code","source":"df_model = pd.get_dummies(df, columns=['cbwd'], dtype=float)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:27.864172Z","iopub.execute_input":"2024-06-27T15:48:27.864455Z","iopub.status.idle":"2024-06-27T15:48:27.883333Z","shell.execute_reply.started":"2024-06-27T15:48:27.864430Z","shell.execute_reply":"2024-06-27T15:48:27.882469Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"n = len(df_model)\n\n# Split 70:20:10 (train:validation:test)\ntrain_df = df_model[0:int(n*0.7)]\ny_train = y[0:int(n*0.7)]\n\nval_df = df_model[int(n*0.7):int(n*0.9)]\ny_val = y[int(n*0.7):int(n*0.9)]\n\ntest_df = df_model[int(n*0.9):]\ny_test = y[int(n*0.9):]\n\nlist(map(lambda x: x.shape, [train_df, y_train, val_df, y_val, test_df, y_test]))","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:27.884509Z","iopub.execute_input":"2024-06-27T15:48:27.884834Z","iopub.status.idle":"2024-06-27T15:48:27.896039Z","shell.execute_reply.started":"2024-06-27T15:48:27.884806Z","shell.execute_reply":"2024-06-27T15:48:27.894998Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[(29229, 10), (29229, 1), (8352, 10), (8352, 1), (4176, 10), (4176, 1)]"},"metadata":{}}]},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(train_df)\n\ntrain_df[train_df.columns] = scaler.transform(train_df[train_df.columns])\nval_df[val_df.columns] = scaler.transform(val_df[val_df.columns])\ntest_df[test_df.columns] = scaler.transform(test_df[test_df.columns])","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:27.900498Z","iopub.execute_input":"2024-06-27T15:48:27.900943Z","iopub.status.idle":"2024-06-27T15:48:27.932585Z","shell.execute_reply.started":"2024-06-27T15:48:27.900912Z","shell.execute_reply":"2024-06-27T15:48:27.931541Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2621853736.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train_df[train_df.columns] = scaler.transform(train_df[train_df.columns])\n/tmp/ipykernel_34/2621853736.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  val_df[val_df.columns] = scaler.transform(val_df[val_df.columns])\n/tmp/ipykernel_34/2621853736.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test_df[test_df.columns] = scaler.transform(test_df[test_df.columns])\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_data(input_width, label_width, shift, df, target_df, target_col):\n    #set_trace()\n    x, y = list(), list()\n    df = df.reset_index(drop=True)\n    target_df = target_df.reset_index(drop=True)\n    df = pd.concat([df, target_df], axis=1)\n    \n    for i in range(0, df.shape[0] - input_width - label_width + 1, shift):\n        try:\n            features = df.loc[i:i+input_width-1].values\n            labels = df.loc[i+input_width:i+input_width+label_width-1, target_col].values\n\n            x.append(features); y.append(labels)\n        except Exception as err:\n            print(err)\n            raise\n        \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:27.933936Z","iopub.execute_input":"2024-06-27T15:48:27.934276Z","iopub.status.idle":"2024-06-27T15:48:27.941980Z","shell.execute_reply.started":"2024-06-27T15:48:27.934248Z","shell.execute_reply":"2024-06-27T15:48:27.941046Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"input_width = 96\nlabel_width = 1\nshift = 1\ntarget_col = 'pm2.5'\n\nx_train, y_train = get_data(input_width, label_width, shift, train_df, y_train, target_col)\nx_val, y_val = get_data(input_width, label_width, shift, val_df, y_val, target_col)\nx_test, y_test = get_data(input_width, label_width, shift, test_df, y_test, target_col)\n\nx_train, y_train = torch.tensor(np.array(x_train)), torch.tensor(np.array(y_train))\nx_val, y_val= torch.tensor(np.array(x_val)), torch.tensor(np.array(y_val))\nx_test, y_test = torch.tensor(np.array(x_test)), torch.tensor(np.array(y_test))\n\nprint(list(map(lambda x: x.shape, [x_train, y_train, x_val, y_val, x_test, y_test])))","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:27.943359Z","iopub.execute_input":"2024-06-27T15:48:27.943715Z","iopub.status.idle":"2024-06-27T15:48:36.089241Z","shell.execute_reply.started":"2024-06-27T15:48:27.943678Z","shell.execute_reply":"2024-06-27T15:48:36.088163Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[torch.Size([29133, 96, 11]), torch.Size([29133, 1]), torch.Size([8256, 96, 11]), torch.Size([8256, 1]), torch.Size([4080, 96, 11]), torch.Size([4080, 1])]\n","output_type":"stream"}]},{"cell_type":"code","source":"class Embedding(nn.Module):\n    def __init__(self, timestep, n_embed):\n        super().__init__()\n        self.dense1 = nn.Linear(timestep, n_embed) \n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        # x: (B, C, T)\n        x = self.dense1(x) # (B, C, T) @ (T, n_embed) -> (B, C, n_embed)\n        x = self.relu(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:36.090494Z","iopub.execute_input":"2024-06-27T15:48:36.090805Z","iopub.status.idle":"2024-06-27T15:48:36.097875Z","shell.execute_reply.started":"2024-06-27T15:48:36.090778Z","shell.execute_reply":"2024-06-27T15:48:36.096887Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.query = nn.Linear(n_embed, head_size)\n        self.key = nn.Linear(n_embed, head_size)\n        self.value = nn.Linear(n_embed, head_size)\n        \n    def forward(self, x):\n        # x : (B, C, D)\n        \n        query = self.query(x) \n        key = self.key(x)\n        value = self.value(x) # (B, C, n_embed) @ (n_embed, head_size) -> (B, C, head_size)\n        \n        wei = query @ key.transpose(-2, -1) # (B, C, head_size) @ (B, head_size, C) -> (B, C, C)\n        wei = F.softmax(wei, dim=-1)\n        \n        out = wei @ value # (B, C, C) @ (B, C, C) -> (B, C, head_size)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:36.099161Z","iopub.execute_input":"2024-06-27T15:48:36.100420Z","iopub.status.idle":"2024-06-27T15:48:36.109280Z","shell.execute_reply.started":"2024-06-27T15:48:36.100392Z","shell.execute_reply":"2024-06-27T15:48:36.108411Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, n_embed):\n        super().__init__()\n        head_size = n_embed // num_heads\n        print(f'Creating {num_heads} heads with size {head_size}')\n        self.mha = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embed, n_embed)\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.mha], dim=-1)\n        out = self.proj(out)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:36.110380Z","iopub.execute_input":"2024-06-27T15:48:36.110664Z","iopub.status.idle":"2024-06-27T15:48:36.124071Z","shell.execute_reply.started":"2024-06-27T15:48:36.110641Z","shell.execute_reply":"2024-06-27T15:48:36.122975Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, n_embed):\n        super().__init__()\n        self.linear1 = nn.Linear(n_embed, n_embed)\n        self.linear2 = nn.Linear(n_embed, n_embed)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:36.125487Z","iopub.execute_input":"2024-06-27T15:48:36.125901Z","iopub.status.idle":"2024-06-27T15:48:36.135142Z","shell.execute_reply.started":"2024-06-27T15:48:36.125875Z","shell.execute_reply":"2024-06-27T15:48:36.134132Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, num_heads, n_embed):\n        super().__init__()\n        self.mha = MultiHeadAttention(num_heads, n_embed)\n        self.ffw = FeedForward(n_embed)\n        self.ln1 = nn.LayerNorm(n_embed)\n        self.ln2 = nn.LayerNorm(n_embed)\n        \n    def forward(self, x):\n        x = self.ln1(x + self.mha(x))\n        x = self.ln2(x + self.ffw(x))\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:36.136229Z","iopub.execute_input":"2024-06-27T15:48:36.136531Z","iopub.status.idle":"2024-06-27T15:48:36.146102Z","shell.execute_reply.started":"2024-06-27T15:48:36.136506Z","shell.execute_reply":"2024-06-27T15:48:36.145178Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, timestep, n_embed, num_heads, N, output, num_variates):\n        super().__init__()\n        self.N = N\n        self.emb = Embedding(timestep, n_embed)\n        self.blocks = nn.ModuleList([Block(num_heads, n_embed) for _ in range(self.N)])\n        self.avg1d = nn.AvgPool1d(n_embed) # num_variates\n        self.output = nn.Linear(num_variates, output) #n_embed\n        \n    def forward(self, x):\n        B = x.size(0)\n        \n        x = x.permute(0, 2, 1)\n        x = self.emb(x)\n        \n        for _ in range(self.N):\n            x = self.blocks[_](x)\n            \n        #set_trace()\n        #x = self.avg1d(x.permute(0, 2, 1)).view(B, -1) \n        x = self.avg1d(x).view(B, -1)\n        x = self.output(x)\n        \n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:36.147329Z","iopub.execute_input":"2024-06-27T15:48:36.147633Z","iopub.status.idle":"2024-06-27T15:48:36.158234Z","shell.execute_reply.started":"2024-06-27T15:48:36.147608Z","shell.execute_reply":"2024-06-27T15:48:36.157174Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:36.159576Z","iopub.execute_input":"2024-06-27T15:48:36.159936Z","iopub.status.idle":"2024-06-27T15:48:36.172239Z","shell.execute_reply.started":"2024-06-27T15:48:36.159901Z","shell.execute_reply":"2024-06-27T15:48:36.171395Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"training_data = CustomDataset(x_train.float(), y_train.float())\ntrain_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n\nval_data = CustomDataset(x_val.float(), y_val.float())\nval_dataloader = DataLoader(val_data, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:36.173574Z","iopub.execute_input":"2024-06-27T15:48:36.174239Z","iopub.status.idle":"2024-06-27T15:48:36.246717Z","shell.execute_reply.started":"2024-06-27T15:48:36.174203Z","shell.execute_reply":"2024-06-27T15:48:36.245546Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"timestep = 96\nnum_variates = 11\nn_embed = 256\nnum_heads = 4\nN = 3\noutput = 1\nlearning_rate = 0.0001\nnum_epochs = 50\ncriterion = nn.L1Loss()\n\nmodel = Encoder(timestep, n_embed, num_heads, N, output, num_variates).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:36.248079Z","iopub.execute_input":"2024-06-27T15:48:36.248462Z","iopub.status.idle":"2024-06-27T15:48:37.857761Z","shell.execute_reply.started":"2024-06-27T15:48:36.248429Z","shell.execute_reply":"2024-06-27T15:48:37.856806Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Creating 4 heads with size 64\nCreating 4 heads with size 64\nCreating 4 heads with size 64\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_one_epoch(model):\n    model.train(True)\n    running_loss = 0\n    \n    for i, data in enumerate(train_dataloader):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        \n        outputs = model(inputs) # fix the shape of outputs (32, 11, 4]) -> should be (32, 1)\n        #set_trace()\n        \n        loss_val = criterion(outputs, labels)\n        loss_val.backward()\n        \n        optimizer.step()\n        \n        running_loss += loss_val.item()\n        \n    epoch_loss = running_loss / (i + 1)\n    \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:37.858945Z","iopub.execute_input":"2024-06-27T15:48:37.859434Z","iopub.status.idle":"2024-06-27T15:48:37.866694Z","shell.execute_reply.started":"2024-06-27T15:48:37.859407Z","shell.execute_reply":"2024-06-27T15:48:37.865497Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def validation(model):\n    model.eval()\n    running_loss = 0\n    \n    with torch.no_grad():\n        for i, data in enumerate(val_dataloader):\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            \n            loss_val = criterion(outputs, labels)\n            running_loss += loss_val.item()\n            \n        val_loss = running_loss / (i + 1)\n        \n    return val_loss","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:37.868063Z","iopub.execute_input":"2024-06-27T15:48:37.868369Z","iopub.status.idle":"2024-06-27T15:48:37.882458Z","shell.execute_reply.started":"2024-06-27T15:48:37.868345Z","shell.execute_reply":"2024-06-27T15:48:37.881407Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"training_loss, validation_loss = list(), list()\n\nfor epoch in range(1, num_epochs+1):\n    #set_trace()\n    epoch_loss = train_one_epoch(model)\n    training_loss.append(epoch_loss)\n    \n    val_loss = validation(model)\n    validation_loss.append(val_loss)\n    \n    print(f'Epoch {epoch} --> Train: {epoch_loss} and Val: {val_loss}')","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:48:37.883620Z","iopub.execute_input":"2024-06-27T15:48:37.883935Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1 --> Train: 100.36525927731024 and Val: 98.02287180109542\nEpoch 2 --> Train: 99.39904496646216 and Val: 96.67081004889437\nEpoch 3 --> Train: 97.70598012293995 and Val: 94.71130495293195\nEpoch 4 --> Train: 95.52678164116769 and Val: 92.26776435024054\nEpoch 5 --> Train: 92.89010216035643 and Val: 89.50990848393403\nEpoch 6 --> Train: 90.05980067247879 and Val: 86.67310433794361\nEpoch 7 --> Train: 87.20020234310012 and Val: 83.90649947824404\nEpoch 8 --> Train: 84.49846964897884 and Val: 81.26753122492354\nEpoch 9 --> Train: 81.97136602810002 and Val: 78.71347384489783\nEpoch 10 --> Train: 79.59131000908225 and Val: 76.27393624948901\nEpoch 11 --> Train: 77.27708609253332 and Val: 73.97830373187398\nEpoch 12 --> Train: 75.13283447952354 and Val: 71.83398387228796\nEpoch 13 --> Train: 73.1680594778218 and Val: 69.95035106451937\nEpoch 14 --> Train: 71.45526600407455 and Val: 68.35282084738562\nEpoch 15 --> Train: 69.9924363974813 and Val: 67.04760783587315\nEpoch 16 --> Train: 68.83021640044798 and Val: 66.06898259007653\nEpoch 17 --> Train: 67.92000238981781 and Val: 65.36607212983361\nEpoch 18 --> Train: 67.27157277464212 and Val: 64.91574540249137\nEpoch 19 --> Train: 66.85923471042537 and Val: 64.66189267653827\nEpoch 20 --> Train: 66.58982118424417 and Val: 64.54088805442633\nEpoch 21 --> Train: 66.44873518791994 and Val: 64.49929793306099\nEpoch 22 --> Train: 63.93202311225833 and Val: 54.17615806964017\nEpoch 23 --> Train: 53.454362651519276 and Val: 49.527383826499765\nEpoch 24 --> Train: 48.930775222610826 and Val: 45.87977360570154\nEpoch 25 --> Train: 45.905044428996014 and Val: 44.448748144992564\nEpoch 26 --> Train: 43.64756738262825 and Val: 41.3400953573774\nEpoch 27 --> Train: 41.50229463462118 and Val: 39.10107714630837\nEpoch 28 --> Train: 39.11836758036776 and Val: 37.92818413963614\nEpoch 29 --> Train: 38.06070477457392 and Val: 34.43409527919089\nEpoch 30 --> Train: 34.51532239945346 and Val: 33.06212545734967\nEpoch 31 --> Train: 32.42847710932911 and Val: 31.052223911581113\nEpoch 32 --> Train: 30.49710847308162 and Val: 28.952859797218974\nEpoch 33 --> Train: 29.068081560564092 and Val: 27.80699965750524\nEpoch 34 --> Train: 27.54111866160622 and Val: 26.63539903478105\nEpoch 35 --> Train: 26.17361569064378 and Val: 25.36244794934295\nEpoch 36 --> Train: 25.04249907168023 and Val: 24.371373723643696\nEpoch 37 --> Train: 24.09842656603498 and Val: 24.097064188284467\nEpoch 38 --> Train: 23.266834309805105 and Val: 22.510306605997012\nEpoch 39 --> Train: 22.551514247638856 and Val: 22.153914897016776\n","output_type":"stream"}]},{"cell_type":"code","source":"# Epoch 50 --> Train: 16.047986085014468 and Val: 14.75692885051402 ::: After permutation\n# Epoch 50 --> Train: 16.505132168498704 and Val: 15.850569702858149 ::: No permutation","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}